{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "705a6225",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "Nyul-Udupa histogram rescaling\n",
    "\n",
    "1. compute landmarks for all\n",
    "2. standard scale = avg landmarks (total sum div by total #inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec891cc",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5c970e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NYU\n",
    "code_src    = \"/gpfs/home/gologr01\"\n",
    "data_src    = \"/gpfs/data/oermannlab/private_data/DeepPit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "84adce18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMich \n",
    "# code src: \"/home/labcomputer/Desktop/Rachel\"\n",
    "# data src: \"../../../../..//media/labcomputer/e33f6fe0-5ede-4be4-b1f2-5168b7903c7a/home/rachel/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e48e7c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders in dset src: ICMB, ABVIB (1).zip, central.xnat.org, ADNI, PPMI, Oasis_long, samir_labels, ACRIN-FMISO-Brain, LGG-1p19qDeletion, REMBRANDT, AIBL, CPTAC-GBM, TCGA-GBM, TCGA-LGG, ABVIB, ABIDE, AIBL.zip\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Paths to (1) code (2) data (3) saved models (4) saved metadata\n",
    "deepPit_src = f\"{code_src}/DeepPit\"\n",
    "obelisk_src = f\"{code_src}/OBELISK\"\n",
    "\n",
    "# saved models, dset metadata\n",
    "model_src  = f\"{data_src}/saved_models\"\n",
    "dsetmd_src = f\"{data_src}/saved_dset_metadata\"\n",
    "\n",
    "# dsets\n",
    "dsets_src    = f\"{data_src}/PitMRdata\"\n",
    "\n",
    "# key,val = dset_name, path to top level dir\n",
    "dset_dict = {\n",
    "    \"ABIDE\"                  : f\"{dsets_src}/ABIDE\",\n",
    "    \"ABVIB\"                  : f\"{dsets_src}/ABVIB/ABVIB\",\n",
    "    \"ADNI1_Complete_1Yr_1.5T\": f\"{dsets_src}/ADNI/ADNI1_Complete_1Yr_1.5T/ADNI\",\n",
    "    \"AIBL\"                   : f\"{dsets_src}/AIBL/AIBL\",\n",
    "    \"ICMB\"                   : f\"{dsets_src}/ICMB/ICBM\",\n",
    "    \"PPMI\"                   : f\"{dsets_src}/PPMI/PPMI\",\n",
    "}\n",
    "\n",
    "# print\n",
    "print(\"Folders in dset src: \", end=\"\"); print(*os.listdir(dsets_src), sep=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "122d6624",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0cc19f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "from transforms import AddChannel, Iso, PadSz\n",
    "\n",
    "# Utilities\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# regex\n",
    "from re import search\n",
    "\n",
    "# Input IO\n",
    "import SimpleITK as sitk\n",
    "import meshio\n",
    "\n",
    "# Numpy and Pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as DF\n",
    "\n",
    "# Fastai + distributed training\n",
    "from fastai import *\n",
    "from fastai.torch_basics import *\n",
    "from fastai.basics import *\n",
    "from fastai.distributed import *\n",
    "\n",
    "# PyTorch\n",
    "from torchvision.models.video import r3d_18\n",
    "from fastai.callback.all import SaveModelCallback\n",
    "from torch import nn\n",
    "\n",
    "# Obelisk\n",
    "sys.path.append(deepPit_src)\n",
    "sys.path.append(obelisk_src)\n",
    "\n",
    "# OBELISK\n",
    "from utils import *\n",
    "from models import obelisk_visceral, obeliskhybrid_visceral\n",
    "\n",
    "# 3D extension to FastAI\n",
    "# from faimed3d.all import *\n",
    "\n",
    "# Helper functions\n",
    "from helpers.preprocess import get_data_dict, paths2objs, folder2objs, seg2mask, mask2bbox, print_bbox, get_bbox_size, print_bbox_size\n",
    "from helpers.general import sitk2np, np2sitk, print_sitk_info, round_tuple, lrange, lmap, get_roi_range, numbers2groups\n",
    "from helpers.viz import viz_axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "072bc5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = []\n",
    "for dset_name in (\"AIBL\", \"ABVIB\", \"ICMB\", \"PPMI\"):\n",
    "    dset_src  = dset_dict[dset_name]\n",
    "    with open(f\"{dsetmd_src}/{dset_name}_fnames.txt\", \"rb\") as f:\n",
    "        fnames.append(pickle.load(f))\n",
    "\n",
    "# flatten\n",
    "fnames = [x for dset in fnames for x in dset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "467b9517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3411\n"
     ]
    }
   ],
   "source": [
    "print(len(fnames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "eacecc54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected: 3284, TODO: 7, Dupl: 120\n"
     ]
    }
   ],
   "source": [
    "corrected = []\n",
    "uncorrected = []\n",
    "multiple    = []\n",
    "\n",
    "def is_corrected(f):\n",
    "    nii_paths = glob.glob(f\"{f}/*corrected_n4.nii\")\n",
    "    \n",
    "    if len(nii_paths) == 1:\n",
    "        corrected.append(nii_paths[0])\n",
    "        return True\n",
    "    \n",
    "    if len(nii_paths) == 0: \n",
    "        uncorrected.append(f)\n",
    "        return False\n",
    "    \n",
    "    if len(nii_paths) > 1: \n",
    "        multiple.append(f)\n",
    "        return True  \n",
    "                \n",
    "for f in fnames:\n",
    "    is_corrected(f)\n",
    "    \n",
    "print(f\"Corrected: {len(corrected)}, TODO: {len(uncorrected)}, Dupl: {len(multiple)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f3cc52b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/gpfs/data/oermannlab/private_data/DeepPit/PitMRdata/AIBL/AIBL/338/MPRAGE_SAG_ISO_p2_ND/2012-10-06_11_32_11.0/S231118',\n",
       " '/gpfs/data/oermannlab/private_data/DeepPit/PitMRdata/AIBL/AIBL/326/MPRAGE_ADNI_confirmed/2013-09-15_08_42_36.0/S236430/corrected_n4.nii')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fnames[0], corrected[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405e9a62",
   "metadata": {},
   "source": [
    "# Get chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "77c4ae62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N_chunks = 66\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    taskid = int(os.getenv('SLURM_ARRAY_TASK_ID'))\n",
    "except:\n",
    "    taskid = 0\n",
    "    \n",
    "n_total = len(corrected)\n",
    "\n",
    "chunk_len = 50    \n",
    "chunks    = [range(i,min(i+chunk_len, n_total)) for i in range(0, n_total, chunk_len)]\n",
    "\n",
    "print(f\"N_chunks = {len(chunks)}\")\n",
    "# print(f\"Array Task ID: {taskid}\")\n",
    "# print(f\"Array ID: {os.getenv('SLURM_ARRAY_TASK_ID')}\")\n",
    "# print(f\"Job ID: {os.getenv('SLURM_JOB_ID')}\")\n",
    "#print(*chunks, sep=\"\\n\")\n",
    "\n",
    "task_chunk = chunks[taskid]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215209e3",
   "metadata": {},
   "source": [
    "# Transform\n",
    "\n",
    "## from FAIMED3D 02_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8140f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from FAIMED3D 02_preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78740421",
   "metadata": {},
   "source": [
    "Piecewise linear histogram matching\n",
    "[1] N. Laszlo G and J. K. Udupa, “On Standardizing the MR Image Intensity Scale,” Magn. Reson. Med., vol. 42, pp. 1072–1081, 1999.\n",
    "\n",
    "[2] M. Shah, Y. Xiao, N. Subbanna, S. Francis, D. L. Arnold, D. L. Collins, and T. Arbel, “Evaluating intensity normalization on MRIs of human brain with multiple sclerosis,” Med. Image Anal., vol. 15, no. 2, pp. 267–282, 2011.\n",
    "\n",
    "Implementation adapted from: https://github.com/jcreinhold/intensity-normalization, ported to pytorch (no use of numpy works in cuda).\n",
    "\n",
    "In contrast to hist_scaled, the piecewise linear histogram matching need pre-specified values for new scale and landmarks. It should be used to normalize a whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3aa99569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a31a7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentile(t, q):\n",
    "    \"\"\"\n",
    "    Return the ``q``-th percentile of the flattened input tensor's data.\n",
    "\n",
    "    CAUTION:\n",
    "     * Needs PyTorch >= 1.1.0, as ``torch.kthvalue()`` is used.\n",
    "     * Values are not interpolated, which corresponds to\n",
    "       ``numpy.percentile(..., interpolation=\"nearest\")``.\n",
    "\n",
    "    :param t: Input tensor.\n",
    "    :param q: Percentile to compute, which must be between 0 and 100 inclusive.\n",
    "    :return: Resulting value (float).\n",
    "\n",
    "    This function is twice as fast as torch.quantile and has no size limitations\n",
    "    \"\"\"\n",
    "    # Note that ``kthvalue()`` works one-based, i.e. the first sorted value\n",
    "    # indeed corresponds to k=1, not k=0! Use float(q) instead of q directly,\n",
    "    # so that ``round()`` returns an integer, even if q is a np.float32.\n",
    "\n",
    "    k = 1 + round(.01 * float(q) * (t.numel() - 1))\n",
    "    result = t.view(-1).kthvalue(k)[0].item()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b62ecc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landmarks(t: torch.Tensor, percentiles: torch.Tensor)->torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns the input's landmarks.\n",
    "\n",
    "    :param t (torch.Tensor): Input tensor.\n",
    "    :param percentiles (torch.Tensor): Peraentiles to calculate landmarks for.\n",
    "    :return: Resulting landmarks (torch.tensor).\n",
    "    \"\"\"\n",
    "    return tensor([get_percentile(t, perc.item()) for perc in percentiles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e75fcad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_one_landmark(input_image, i_min=1, i_max=99, i_s_min=1, i_s_max=100, l_percentile=10, u_percentile=90, step=10):\n",
    "    \"\"\"\n",
    "    determine the standard scale for the set of images\n",
    "    Args:\n",
    "        inputs (list or L): set of TensorDicom3D objects which are to be normalized\n",
    "        i_min (float): minimum percentile to consider in the images\n",
    "        i_max (float): maximum percentile to consider in the images\n",
    "        i_s_min (float): minimum percentile on the standard scale\n",
    "        i_s_max (float): maximum percentile on the standard scale\n",
    "        l_percentile (int): middle percentile lower bound (e.g., for deciles 10)\n",
    "        u_percentile (int): middle percentile upper bound (e.g., for deciles 90)\n",
    "        step (int): step for middle percentiles (e.g., for deciles 10)\n",
    "    Returns:\n",
    "        standard_scale (np.ndarray): average landmark intensity for images\n",
    "        percs (np.ndarray): array of all percentiles used\n",
    "    \"\"\"\n",
    "    percs = torch.cat([torch.tensor([i_min]),\n",
    "                       torch.arange(l_percentile, u_percentile+1, step),\n",
    "                       torch.tensor([i_max])], dim=0)\n",
    "   \n",
    "    mask_data = input_image > input_image.mean()\n",
    "    masked = input_image[mask_data]\n",
    "    landmarks = get_landmarks(masked, percs)\n",
    "    min_p = get_percentile(masked, i_min)\n",
    "    max_p = get_percentile(masked, i_max)\n",
    "    new_landmarks = landmarks.interp_1d(torch.FloatTensor([i_s_min, i_s_max]),\n",
    "                                        torch.FloatTensor([min_p, max_p]))\n",
    "    return new_landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6ef298b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sum_landmarks(inputs, i_min=1, i_max=99, i_s_min=1, i_s_max=100, l_percentile=10, u_percentile=90, step=10):\n",
    "    \"\"\"\n",
    "    determine the standard scale for the set of images\n",
    "    Args:\n",
    "        inputs (list or L): set of TensorDicom3D objects which are to be normalized\n",
    "        i_min (float): minimum percentile to consider in the images\n",
    "        i_max (float): maximum percentile to consider in the images\n",
    "        i_s_min (float): minimum percentile on the standard scale\n",
    "        i_s_max (float): maximum percentile on the standard scale\n",
    "        l_percentile (int): middle percentile lower bound (e.g., for deciles 10)\n",
    "        u_percentile (int): middle percentile upper bound (e.g., for deciles 90)\n",
    "        step (int): step for middle percentiles (e.g., for deciles 10)\n",
    "    Returns:\n",
    "        standard_scale (np.ndarray): average landmark intensity for images\n",
    "        percs (np.ndarray): array of all percentiles used\n",
    "    \"\"\"\n",
    "    percs = torch.cat([torch.tensor([i_min]),\n",
    "                       torch.arange(l_percentile, u_percentile+1, step),\n",
    "                       torch.tensor([i_max])], dim=0)\n",
    "    standard_scale = torch.zeros(len(percs))\n",
    "\n",
    "    for input_image in inputs:\n",
    "        mask_data = input_image > input_image.mean()\n",
    "        masked = input_image[mask_data]\n",
    "        landmarks = get_landmarks(masked, percs)\n",
    "        min_p = get_percentile(masked, i_min)\n",
    "        max_p = get_percentile(masked, i_max)\n",
    "        new_landmarks = landmarks.interp_1d(torch.FloatTensor([i_s_min, i_s_max]),\n",
    "                                            torch.FloatTensor([min_p, max_p]))\n",
    "        standard_scale += new_landmarks\n",
    "    #standard_scale = standard_scale / len(inputs)\n",
    "    return standard_scale, percs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c00a284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def path2tensor(mr_path):\n",
    "    mr = sitk.ReadImage(mr_path, sitk.sitkFloat32)\n",
    "    return torch.transpose(torch.tensor(sitk.GetArrayFromImage(mr)), 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4795f1",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "eb1944c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "/gpfs/data/oermannlab/private_data/DeepPit/PitMRdata/AIBL/AIBL/326/MPRAGE_ADNI_confirmed/2013-09-15_08_42_36.0/S236430/corrected_n4.nii\n",
      "/gpfs/data/oermannlab/private_data/DeepPit/PitMRdata/AIBL/AIBL/1413/MPRAGE_ADNI_confirmed/2013-06-29_11_43_08.0/S236380/corrected_n4.nii\n",
      "/gpfs/data/oermannlab/private_data/DeepPit/PitMRdata/AIBL/AIBL/1595/MPRAGE_ADNI_confirmed/2013-12-12_12_07_53.0/S235320/corrected_n4.nii\n",
      "/gpfs/data/oermannlab/private_data/DeepPit/PitMRdata/AIBL/AIBL/1460/MPRAGE_ADNI_confirmed/2013-06-07_16_11_03.0/S232401/corrected_n4.nii\n",
      "/gpfs/data/oermannlab/private_data/DeepPit/PitMRdata/AIBL/AIBL/773/MPRAGE_SAG_ISO_p2/2012-05-03_10_38_50.0/S236142/corrected_n4.nii\n"
     ]
    }
   ],
   "source": [
    "corrected_chunk = [corrected[i] for i in task_chunk]\n",
    "print(len(corrected_chunk), *corrected_chunk, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "35fba627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 /gpfs/data/oermannlab/private_data/DeepPit/PitMRdata/AIBL/AIBL/326/MPRAGE_ADNI_confirmed/2013-09-15_08_42_36.0/S236430/corrected_n4.nii\n",
      "Elapsed: 0.69 s\n",
      "1 /gpfs/data/oermannlab/private_data/DeepPit/PitMRdata/AIBL/AIBL/1413/MPRAGE_ADNI_confirmed/2013-06-29_11_43_08.0/S236380/corrected_n4.nii\n",
      "Elapsed: 0.76 s\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "# from FAIMED3D 02_preprocessing\n",
    "# and https://simpleitk.readthedocs.io/en/master/link_N4BiasFieldCorrection_docs.html\n",
    "\n",
    "count = 0\n",
    "for mr_path in corrected_chunk: \n",
    "    \n",
    "    #start = time.time()\n",
    "    try:\n",
    "        # print\n",
    "        print(count, mr_path, flush=True)\n",
    "        count += 1\n",
    "\n",
    "        # Read in image\n",
    "        inputTensor = path2tensor(mr_path) \n",
    "\n",
    "        # Get landmarks\n",
    "        landmark = find_one_landmark(inputTensor)\n",
    "\n",
    "        # write image\n",
    "        corrected_fn = f\"{Path(mr_path).parent}/landmark.pt\"\n",
    "        torch.save(landmark, corrected_fn)\n",
    "    except Exception as e:\n",
    "        #raise(e)\n",
    "        print(\"Skipped: \", mr_path)\n",
    "    \n",
    "    #elapsed = time.time() - start\n",
    "    #print(f\"Elapsed: {elapsed:0.2f} s\")\n",
    "\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780a92be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
