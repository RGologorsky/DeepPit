{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "Run on Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#GPU = 1, #CPU = 64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "gpu_count = torch.cuda.device_count()\n",
    "cpu_count = os.cpu_count()\n",
    "print(\"#GPU = {0:d}, #CPU = {1:d}\".format(gpu_count, cpu_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader params\n",
    "\n",
    "subset_size = 330\n",
    "\n",
    "test_pct  = 1 - float(subset_size)/335\n",
    "bs        = 20\n",
    "nepochs   = 3\n",
    "num_workers = 32\n",
    "\n",
    "iso       = 3\n",
    "maxs      = [87, 90, 90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "\n",
    "# CODE (DeepPit, OBELISK, etc)\n",
    "code_src     = \"/workspace\"\n",
    "deepPit_src  = f\"{code_src}/DeepPit\"\n",
    "obelisk_src  = f\"{code_src}/OBELISK\"\n",
    "\n",
    "# DATA (training, ABIDE, etc)\n",
    "todd_data_src = \"../../../../..//media/labcomputer/e33f6fe0-5ede-4be4-b1f2-5168b7903c7a/home/rachel/\"\n",
    "olab_data_src = \"/gpfs/data/oermannlab/private_data/DeepPit/\"\n",
    "docker_data_src = \"../PitMRdata/Labels/ABIDE\"\n",
    "\n",
    "code_src      = \"/gpfs/home/gologr01/DeepPit/\"\n",
    "curr_data_src = olab_data_src\n",
    "\n",
    "def change_src(fn, old_prefix=todd_data_src, new_prefix=olab_data_src):\n",
    "    return new_prefix + fn[len(old_prefix):]\n",
    "\n",
    "# path to training data\n",
    "train_src = \"/gpfs/data/oermannlab/private_data/DeepPit/PitMRdata/samir_labels\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#GPU = 1, #CPU = 64\n",
      "Index 0, GPU GeForce RTX 3090     RAM Free: 23872MB | Used: 393MB | Util   2% | Total 24265MB\n"
     ]
    }
   ],
   "source": [
    "# clear cache\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(\"#GPU = {0:d}, #CPU = {1:d}\".format(torch.cuda.device_count(), os.cpu_count()))\n",
    "\n",
    "# print GPU stats\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "for i,gpu in enumerate(GPUs):\n",
    "    print(\"Index {0:d}, GPU {1:20s} RAM Free: {2:.0f}MB | Used: {3:.0f}MB | Util {4:3.0f}% | Total {5:.0f}MB\".format(i, gpu.name, gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from transforms import AddChannel, Iso, PadSz\n",
    "\n",
    "# Utilities\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Fastai\n",
    "from fastai import *\n",
    "from fastai.torch_basics import *\n",
    "from fastai.basics import *\n",
    "\n",
    "# PyTorch\n",
    "from torchvision.models.video import r3d_18\n",
    "from fastai.callback.all import SaveModelCallback\n",
    "from torch import nn\n",
    "\n",
    "# 3D extension to FastAI\n",
    "# from faimed3d.all import *\n",
    "\n",
    "# Input IO\n",
    "import SimpleITK as sitk\n",
    "import meshio\n",
    "\n",
    "# Numpy and Pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as DF\n",
    "\n",
    "# Helper functions\n",
    "from helpers.preprocess import get_data_dict, paths2objs, folder2objs, seg2mask, mask2bbox, print_bbox, get_bbox_size, print_bbox_size\n",
    "from helpers.general import sitk2np, np2sitk, print_sitk_info, round_tuple, lrange, lmap, get_roi_range, numbers2groups\n",
    "from helpers.viz import viz_axis\n",
    "\n",
    "import sys\n",
    "sys.path.append(deepPit_src)\n",
    "sys.path.append(obelisk_src)\n",
    "\n",
    "# OBELISK\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fastai.distributed import *\n",
    "# import argparse\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument(\"--local_rank\", type=int)\n",
    "# args = parser.parse_args()\n",
    "# torch.cuda.set_device(args.local_rank)\n",
    "# torch.distributed.init_process_group(backend='nccl', init_method='env://')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Source = path to labels (segmentation)\n",
    "2. data dict[foldername] = (path to MR, path to Segm tensor)\n",
    "    \n",
    "Special subsets:\n",
    "1. *training*: small subset of all labelled items (quick epoch w/ 100 instead of 335 items).\n",
    "2. *unique*: subset of items with unique size, spacing, and orientation (quickly evaluate resize vs. istropic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: Fri Jun 18 17:30:58 2021\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "model_time = time.ctime() # 'Mon Oct 18 13:35:29 2010'\n",
    "print(f\"Time: {model_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders in train path: 50213-50312.zip, 50373-50453, 50313-50372, 50155-50212.zip, 50213-50312, 50155-50212, 50313-50372.zip, 50373-50453.zip, 50002-50153, 50002-50153.zip\n",
      "Folders  ['50373-50453', '50313-50372', '50213-50312', '50155-50212', '50002-50153']\n",
      "Total 335 items in dataset.\n",
      "Training subset of 330 items.\n",
      "Test subset of 5 items.\n",
      "Model name: iso_3mm_pad_87_90_90_bs_20_subset_330_epochs_20_time_Fri Jun 18 17:30:58 2021\n",
      "Total 335 items in dataset.\n",
      "Training subset of 330 items.\n",
      "Unique subset of 28 items.\n"
     ]
    }
   ],
   "source": [
    "# labelled train data\n",
    "train_src = docker_data_src\n",
    "\n",
    "# print\n",
    "print(\"Folders in train path: \", end=\"\"); print(*os.listdir(train_src), sep=\", \")\n",
    "\n",
    "# get data\n",
    "data = {}\n",
    "folders = os.listdir(train_src)\n",
    "\n",
    "# filter .zip\n",
    "folders = [folder for folder in folders if not folder.endswith(\".zip\")]\n",
    "print(\"Folders \", folders)\n",
    "\n",
    "for folder in folders: data.update(get_data_dict(f\"{train_src}/{folder}/{folder}\"))\n",
    "\n",
    "# all items\n",
    "items = list(data.values())\n",
    "\n",
    "# MR files: unique sz, sp, dir\n",
    "with open(f'{deepPit_src}/saved_metadata/unique_sz_sp_dir.pkl', 'rb') as f:\n",
    "    unique = pickle.load(f)\n",
    "\n",
    "# Create (MR path, Segm path) item from MR path\n",
    "def get_folder_name(s):\n",
    "    start = s.index(\"samir_labels/\")\n",
    "    s = s[start + len(\"samir_labels/50373-50453/\"):]\n",
    "    return s[0:s.index(\"/\")]\n",
    "\n",
    "# get unique\n",
    "unique = [(change_src(mr), data[get_folder_name(mr)][1]) for mr in unique]\n",
    "\n",
    "# subset\n",
    "subset_idxs, test_idxs = RandomSplitter(valid_pct=test_pct)(items)\n",
    "subset = [items[i] for i in subset_idxs]\n",
    "test   = [items[i] for i in test_idxs]\n",
    "\n",
    "# print\n",
    "print(f\"Total {len(items)} items in dataset.\")\n",
    "print(f\"Training subset of {len(subset)} items.\")\n",
    "print(f\"Test subset of {len(test)} items.\")\n",
    "\n",
    "# model name\n",
    "model_name = f\"iso_{iso}mm_pad_{maxs[0]}_{maxs[1]}_{maxs[2]}_bs_{bs}_subset_{len(subset)}_epochs_{nepochs}_time_{model_time}\"\n",
    "print(f\"Model name: {model_name}\")\n",
    "\n",
    "# save test set indices\n",
    "with open(f'{deepPit_src}/model_test_sets/{model_name}_test_items.pkl', 'wb') as f:\n",
    "    pickle.dump(list(test), f)\n",
    "    \n",
    "# print\n",
    "print(f\"Total {len(items)} items in dataset.\")\n",
    "print(f\"Training subset of {len(subset)} items.\")\n",
    "print(f\"Unique subset of {len(unique)} items.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model name: iso_3mm_pad_87_90_90_bs_20_subset_330_epochs_20_time_Fri Jun 18 17:30:58 2021\n"
     ]
    }
   ],
   "source": [
    "# model name\n",
    "model_name = f\"iso_3mm_pad_87_90_90_bs_{bs}_subset_{len(subset)}_epochs_{nepochs}_time_{model_time}\"\n",
    "print(f\"Model name: {model_name}\"),\n",
    "\n",
    "# save test set indices\\n\",\n",
    "with open(f'{deepPit_src}/model_test_sets/{model_name}_test_items.pkl', 'wb') as f:\n",
    "    pickle.dump(list(test), f)\n",
    "      \n",
    "# with open(f\"model_test_sets/{model_name}_test_items.pkl\", 'rb') as f:\n",
    "#     test = pickle.load(f)\n",
    "# print(test[0]), print(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms\n",
    "\n",
    "1. Isotropic 3mm or Resize to 50x50x50 dimensions\n",
    "2. Crop/Pad to common dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "\n",
    "# tfms = [Iso(3)]\n",
    "# tls = TfmdLists(unique, tfms)\n",
    "\n",
    "# start = time.time()\n",
    "# iso_szs = [mr.shape for mr,mk in tls]\n",
    "# elapsed = time.time() - start\n",
    "\n",
    "# print(f\"Elapsed: {elapsed} s for {len(unique)} items.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "# iso_szs = [mr.shape for mr,mk in tls]\n",
    "# elapsed = time.time() - start\n",
    "\n",
    "# print(f\"Elapsed: {elapsed} s for {len(unique)} items.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(*[f\"{get_folder_name(mr)}: {tuple(sz)}\" for (mr,mk),sz in zip(unique, iso_szs)], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maxs = [int(x) for x in torch.max(torch.tensor(iso_szs), dim=0).values]\n",
    "# print(\"Maxs: \", maxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# iso_items = list(tls[0:2])\n",
    "\n",
    "# # tfms\n",
    "# pad_tfms = [PadSz(maxs)]\n",
    "\n",
    "# # tls\n",
    "# pad_tls = TfmdLists(iso_items, pad_tfms)\n",
    "\n",
    "# pad_tls[0][0].shape, pad_tls[1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloaders\n",
    "\n",
    "TODO augmentations.\n",
    "\n",
    "- dset = tfms applied to items\n",
    "- splits into training/valid\n",
    "- bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 264, Valid: 66\n",
      "Elapsed time: 2.2636847496032715 s for 330 items\n",
      "<class 'tuple'> torch.Size([20, 1, 87, 90, 90]) torch.Size([20, 1, 87, 90, 90])\n",
      "13 4\n"
     ]
    }
   ],
   "source": [
    "# time it\n",
    "start = time.time()\n",
    "\n",
    "# splits\n",
    "splits = RandomSplitter(seed=42)(subset)\n",
    "print(f\"Training: {len(splits[0])}, Valid: {len(splits[1])}\")\n",
    "\n",
    "# tfms\n",
    "tfms = [Iso(3), PadSz(maxs)]\n",
    "\n",
    "# tls\n",
    "tls = TfmdLists(items, tfms, splits=splits)\n",
    "\n",
    "# dls\n",
    "dls = tls.dataloaders(bs=bs, after_batch=AddChannel(), num_workers=num_workers)\n",
    "\n",
    "# GPU\n",
    "dls = dls.cuda()\n",
    "\n",
    "# end timer\n",
    "elapsed = time.time() - start\n",
    "print(f\"Elapsed time: {elapsed} s for {len(subset)} items\")\n",
    "\n",
    "# test get one batch\n",
    "b = dls.one_batch()\n",
    "print(type(b), b[0].shape, b[1].shape)\n",
    "print(len(dls.train), len(dls.valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metric\n",
    "\n",
    "Linear combination of Dice and Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(input, target):\n",
    "    iflat = input.contiguous().view(-1)\n",
    "    tflat = target.contiguous().view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    return ((2. * intersection) /\n",
    "           (iflat.sum() + tflat.sum()))\n",
    "\n",
    "def dice_score(input, target):\n",
    "    return dice(input.argmax(1), target)\n",
    "\n",
    "def dice_loss(input, target): \n",
    "    return 1 - dice(input.softmax(1)[:, 1], target)\n",
    "\n",
    "def loss(input, target):\n",
    "    return dice_loss(input, target) + nn.CrossEntropyLoss()(input, target[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OBELISK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start = time.time()\n",
    "\n",
    "# segs = torch.cat([tl[1] for tl in dls.train],0)\n",
    "# print(segs.shape)\n",
    "\n",
    "# elapsed = time.time() - start\n",
    "\n",
    "# print(f\"Elapsed time: {elapsed} s for {len(segs)} items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_weight = torch.sqrt(1.0/(torch.bincount(segs.view(-1)).float()))\n",
    "# class_weight = class_weight/class_weight.mean()\n",
    "# class_weight[0] = 0.5\n",
    "# np.set_printoptions(formatter={'float': '{: 0.2f}'.format})\n",
    "# print('inv sqrt class_weight',class_weight.data.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import my_ohem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_weight = torch.load(\"saved_metadata/class_weights.pt\")\n",
    "# class_weights = [0, pos_weight]\n",
    "\n",
    "# # inv\n",
    "# class_weights [1.0/x for x in class_wei]\n",
    "# my_criterion = my_ohem(.25,[0, pos_weight]) #.cuda())#0.25 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obelisk_loss_fn(predict, target): return my_criterion(F.log_softmax(predict,dim=1),target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipython nbconvert --to python  '6 - Dataloaders- NB - Simple-Copy1.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBELISK-NET from github\n",
    "from models import obelisk_visceral, obeliskhybrid_visceral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_res = maxs\n",
    "\n",
    "learn = Learner(dls=dls, \\\n",
    "                model=obeliskhybrid_visceral(num_labels=2, full_res=full_res), \\\n",
    "                loss_func= loss, #DiceLoss(), #nn.CrossEntropyLoss(), \\\n",
    "                metrics = dice_score, \\\n",
    "                model_dir = code_src + \"models\", \\\n",
    "                cbs = [SaveModelCallback(monitor='dice_score', fname=model_name, with_opt=True)])\n",
    "\n",
    "# SaveModelCallback: model_dir = \"./models\", cbs = [SaveModelCallback(monitor='dice_score')]\n",
    "\n",
    "# GPU\n",
    "learn.model = learn.model.cuda()\n",
    "\n",
    "#learn = learn.to_distributed(args.local_rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test:\n",
    "\n",
    "# #dls.device = \"cpu\"\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# x,y = dls.one_batch()\n",
    "# #x,y = to_cpu(x), to_cpu(y)\n",
    "\n",
    "# pred = learn.model(x)\n",
    "# loss = learn.loss_func(pred, y)\n",
    "\n",
    "# elapsed = time.time() - start\n",
    "\n",
    "# print(f\"Elapsed: {elapsed} s\")\n",
    "# print(\"Batch: x,y\")\n",
    "# print(type(x), x.shape, x.dtype, \"\\n\", type(y), y.shape, y.dtype)\n",
    "\n",
    "# print(\"Pred shape\")\n",
    "# print(type(pred), pred.shape, pred.dtype)\n",
    "\n",
    "# print(\"Loss\")\n",
    "# print(loss)\n",
    "# print(learn.loss_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE learn.fit one cycle\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>dice_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.552306</td>\n",
       "      <td>1.504593</td>\n",
       "      <td>0.011449</td>\n",
       "      <td>00:14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better model found at epoch 0 with dice_score value: 0.011448514647781849.\n"
     ]
    }
   ],
   "source": [
    "print(\"PRE learn.fit one cycle\")\n",
    "with learn.distrib_ctx():\n",
    "    learn.fit_one_cycle(1, 3e-3, wd = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"unfreeze, learn 50\")\n",
    "learn.unfreeze()\n",
    "with learn.distrib_ctx():\n",
    "    learn.fit_one_cycle(nepochs, 3e-3, wd = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.save('iso_3mm_pad_87_90_90_subset_50_epochs_50')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"unfreeze, learn 50\")\n",
    "# learn.unfreeze()\n",
    "# learn.fit_one_cycle(50, 1e-3, wd = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testmask = torch.tensor([[[False, False, False], [False, False, False], [True, True, True]],\n",
    "#                        [[False, False, False], [False, False, True], [True, True, True]],\n",
    "#                        [[False, False, False], [False, False, False], [False, False, False]]])\n",
    "# testmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testmaskN = np.array(testmask)\n",
    "# testmaskN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maskT = testmask.type(torch.BoolTensor)\n",
    "\n",
    "# iT = torch.any(maskT, dim=(1,2))\n",
    "# jT = torch.any(maskT, dim=(0,2))\n",
    "# kT = torch.any(maskT, dim=(0,1))\n",
    "\n",
    "# iminT, imaxT = torch.where(iT)[0][[0, -1]]\n",
    "# jminT, jmaxT = torch.where(jT)[0][[0, -1]]\n",
    "# kminT, kmaxT = torch.where(kT)[0][[0, -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maskN = np.array(testmask).astype(bool)\n",
    "    \n",
    "# iN = np.any(maskN, axis=(1, 2))\n",
    "# jN = np.any(maskN, axis=(0, 2))\n",
    "# kN = np.any(maskN, axis=(0, 1))\n",
    "\n",
    "# iminN, imaxN = np.where(iN)[0][[0, -1]]\n",
    "# jminN, jmaxN = np.where(jN)[0][[0, -1]]\n",
    "# kminN, kmaxN = np.where(kN)[0][[0, -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maskT.shape, maskN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(iT)\n",
    "# print(jT)\n",
    "# print(kT)\n",
    "# print([x for x in (iminT, imaxT, jminT, jmaxT, kminT, kmaxT)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(iN)\n",
    "# print(jN)\n",
    "# print(kN)\n",
    "# print([int(x) for x in (iminN, imaxN, jminN, jmaxN, kminN, kmaxN)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def torch_mask2bbox(mask):\n",
    "#         mask = mask.type(torch.BoolTensor)\n",
    "\n",
    "#         i = torch.any(mask, dim=0)\n",
    "#         j = torch.any(mask, dim=1)\n",
    "#         k = torch.any(mask, dim=2)\n",
    "\n",
    "#         imin, imax = torch.where(i)[0][[0, -1]]\n",
    "#         jmin, jmax = torch.where(j)[0][[0, -1]]\n",
    "#         kmin, kmax = torch.where(k)[0][[0, -1]]\n",
    "\n",
    "#         # inclusive idxs\n",
    "#         return imin, imax+1, jmin, jmax+1, kmin, kmax+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
