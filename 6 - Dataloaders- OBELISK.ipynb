{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "unable-instruction",
   "metadata": {},
   "source": [
    "# Goal\n",
    "\n",
    "The goal of this notebook is to train a **OBELISK-NET** to output binary mask representing the sella turcica ROI.\n",
    "\n",
    "With gratitude to \n",
    "- https://github.com/mattiaspaul/OBELISK/blob/master/models.py\n",
    "- https://github.com/kbressem/faimed3d/blob/main/examples/3d_segmentation.md\n",
    "\n",
    "TODO\n",
    "- Augmentations: flip, orientation\n",
    "- Intensity normalization: N4 bias correction, hist bin matching, tissue intensity,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0187b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OBELISK-NET from github\n",
    "import sys\n",
    "sys.path.append('/home/labcomputer/Desktop/Rachel/OBELISK')\n",
    "from models import obelisk_visceral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c149874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device 0 : b'GeForce GTX 1080 Ti'\n",
      "Device 1 : b'GeForce GTX 1080'\n",
      "gpu: 0%, gpu-mem: 0%\n",
      "is cuda available? True\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Check GPU stats\n",
    "\n",
    "from pynvml import *\n",
    "nvmlInit()\n",
    "try:\n",
    "    deviceCount = nvmlDeviceGetCount()\n",
    "    for i in range(deviceCount):\n",
    "        handle = nvmlDeviceGetHandleByIndex(i)\n",
    "        print(\"Device\", i, \":\", nvmlDeviceGetName(handle))\n",
    "except NVMLError as error:\n",
    "    print(error)\n",
    "    \n",
    "# https://docs.fast.ai/dev/gpu.html\n",
    "import nvidia_smi\n",
    "\n",
    "nvidia_smi.nvmlInit()\n",
    "handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
    "# card id 0 hardcoded here, there is also a call to get all available card ids, so we could iterate\n",
    "\n",
    "res = nvidia_smi.nvmlDeviceGetUtilizationRates(handle)\n",
    "print(f'gpu: {res.gpu}%, gpu-mem: {res.memory}%')\n",
    "\n",
    "import torch\n",
    "print(\"is cuda available?\", torch.cuda.is_available() )\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.set_device(0)\n",
    "\n",
    "# hm\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81bf7d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27aa128",
   "metadata": {},
   "source": [
    "# Data Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7031ab",
   "metadata": {},
   "source": [
    "Set path to where data is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "775df797",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ebc81c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ABIDE', 'PPMI', 'ABVIB', 'samir_labels']\n",
      "['50155-50212', '50373-50453', '50002-50153', '50213-50312', '50313-50372']\n"
     ]
    }
   ],
   "source": [
    "# Get path to 4 TB HD\n",
    "\n",
    "# /media/labcomputer/e33f6fe0-5ede-4be4-b1f2-5168b7903c7a/home\n",
    "\n",
    "# wsl: /home/rgologorsky/DeepPit\n",
    "hd_path = \"../\" * 5 + \"/media/labcomputer/e33f6fe0-5ede-4be4-b1f2-5168b7903c7a\" + \"/home/rachel/PitMRdata\"\n",
    "\n",
    "# all folders in HD\n",
    "all_folders = os.listdir(hd_path)\n",
    "\n",
    "print(all_folders)\n",
    "\n",
    "# labels\n",
    "print(os.listdir(f\"{hd_path}/samir_labels\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02841785",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "rough-climate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "from faimed3d.all import *\n",
    "from fastai import *\n",
    "from torchvision.models.video import r3d_18\n",
    "from fastai.callback.all import SaveModelCallback\n",
    "from torch import nn\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import meshio\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame as DF\n",
    "\n",
    "from helpers_preprocess import get_data_dict, paths2objs, folder2objs, seg2mask\n",
    "\n",
    "from helpers_general import sitk2np, print_sitk_info, round_tuple, lrange, lmap, get_roi_range, numbers2groups\n",
    "\n",
    "# imports\n",
    "from helpers_general import sitk2np, np2sitk, round_tuple, lrange, get_roi_range, numbers2groups\n",
    "from helpers_preprocess import mask2bbox, print_bbox, get_bbox_size, print_bbox_size, get_data_dict, folder2objs\n",
    "from helpers_viz import viz_axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959ca43b",
   "metadata": {},
   "source": [
    "# Get Items\n",
    "\n",
    "Item = (path to MR, path to Segmentation obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee44fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_sz = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72985d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data dict: 335 items.\n",
      "Subset data dict: 10 items.\n"
     ]
    }
   ],
   "source": [
    "# get items = (mask_fn, nii_fn)\n",
    "train_path = f\"{hd_path}/samir_labels\"\n",
    "folders = os.listdir(train_path)\n",
    "\n",
    "# get all items\n",
    "data_dict_full = {}\n",
    "for folder in folders:\n",
    "    data_dict_full.update(get_data_dict(f\"{train_path}/{folder}\"))\n",
    "\n",
    "items_full = list(data_dict_full.values())\n",
    "\n",
    "# subset 300 for training/valid; 35 for test\n",
    "rand_idx = torch.randperm(subset_sz)\n",
    "items = np.array(items_full)[rand_idx]\n",
    "\n",
    "print(f\"Full data dict: {len(data_dict_full)} items.\")\n",
    "print(f\"Subset data dict: {len(items)} items.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdf8ee3",
   "metadata": {},
   "source": [
    "# Transforms\n",
    "\n",
    "- PathToSITK (*convert paths to SITK obj*)\n",
    "- Resize (*common size, isotropic spacing*)\n",
    "- ToTensor (*convert to Pytorch tensor*)\n",
    "- TensorSlice & Center Crop (*slice 3d tensor to center part containing sella*)\n",
    "- Normalize (*scale image intensities? - diff tissues diff intensities?*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "59d5815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoAll(ItemTransform):\n",
    "    \n",
    "    def __init__(self, new_sp = 1):\n",
    "        self.new_sp = new_sp\n",
    "        \n",
    "    def encodes(self, x):\n",
    "        # get sitk objs\n",
    "        im_path, segm_path = x\n",
    "        folder  = Path(segm_path).parent.name\n",
    "        ras_adj = int(folder) in range(50455, 50464)\n",
    "\n",
    "        mr         = sitk.ReadImage(im_path, sitk.sitkFloat32)\n",
    "        segm       = meshio.read(segm_path)\n",
    "        mask_arr   = seg2mask(mr, segm, ras_adj)\n",
    "\n",
    "        # resize so isotropic spacing\n",
    "        orig_sp = mr.GetSpacing()\n",
    "        orig_sz = mr.GetSize()\n",
    "        new_sz = [int(round(osz*ospc/self.new_sp)) for osz,ospc in zip(orig_sz, orig_sp)]\n",
    "\n",
    "        im = torch.swapaxes(torch.tensor(sitk.GetArrayFromImage(mr)), 0, 2)\n",
    "        mk = torch.tensor(mask_arr).float()\n",
    "\n",
    "        while im.ndim < 5: \n",
    "            im = im.unsqueeze(0)\n",
    "            mk = mk.unsqueeze(0)\n",
    "\n",
    "        return F.interpolate(im, size = new_sz, mode = 'trilinear', align_corners=False).squeeze(), \\\n",
    "                F.interpolate(mk, size = new_sz, mode = 'nearest').squeeze().long()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-attitude",
   "metadata": {},
   "source": [
    "# Crop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe42841d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# crop center\n",
    "class CenterCropTfm(Transform):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        \n",
    "    def encodes(self, arr):\n",
    "        return self.cropND(arr, self.size)\n",
    "    \n",
    "    # https://stackoverflow.com/questions/39382412/crop-center-portion-of-a-numpy-image\n",
    "    @staticmethod\n",
    "    def cropND(img, bounding):\n",
    "        start = tuple(map(lambda a, da: a//2-da//2, img.shape, bounding))\n",
    "        end = tuple(map(operator.add, start, bounding))\n",
    "        slices = tuple(map(slice, start, end))\n",
    "        return img[slices]\n",
    "    \n",
    "# crop by coords\n",
    "class CropBBox(Transform):\n",
    "    def __init__(self, bbox):\n",
    "        self.bbox = bbox\n",
    "    \n",
    "    def encodes(self, arr):\n",
    "        imin, imax, jmin, jmax, kmin, kmax = self.bbox\n",
    "        cropped = arr[imin:imax, jmin:jmax, kmin:kmax]\n",
    "        \n",
    "        # pad if needed\n",
    "        new_size = [imax-imin, jmax-jmin, kmax-kmin]\n",
    "        \n",
    "        pad = [x-y for x,y in zip(new_size, arr.shape)]\n",
    "        pad = [a for amt in pad for a in (amt//2, amt-amt//2)]\n",
    "        pad.reverse()\n",
    "        \n",
    "        return F.pad(arr, pad, mode='constant', value=0)\n",
    "    \n",
    "\n",
    "# add padding\n",
    "class Pad(Transform):\n",
    "    def __init__(self, sz):\n",
    "        self.sz = sz\n",
    "        \n",
    "    def encodes(self, arr):\n",
    "        pad = [x-y for x,y in zip(self.sz, arr.shape)]\n",
    "        pad = [a for amt in pad for a in (amt//2, amt-amt//2)]\n",
    "        pad.reverse()\n",
    "        \n",
    "        return F.pad(arr, pad, mode='constant', value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee03ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from faimed3d 03_transforms.ipynb\n",
    "def resize_3d(t, size, scale_factor=None,\n",
    "              mode='trilinear', align_corners=True, recompute_scale_factor=None):\n",
    "    '''\n",
    "    A function to resize a 3D image using torch.nn.functional.interpolate\n",
    "\n",
    "    Args:\n",
    "        t (Tensor): a 3D or 4D Tensor to be resized\n",
    "        size (int): a tuple with the new x,y,z dimensions of the tensor after resize\n",
    "        scale_factor, mode, align_corners, recompute_scale_factor: args from F.interpolate\n",
    "    Returns:\n",
    "        A new `Tensor` with Tensor.size = size\n",
    "    '''\n",
    "    old_sz = t.shape[-3:]\n",
    "    if hasattr(t, '_metadata'):\n",
    "        spacing = t.get_spacing()\n",
    "        if scale_factor is None:\n",
    "            change_factors = list(map(operator.truediv, old_sz,  size))\n",
    "            change_factors.reverse() # spacing is H x W x D, but tensor is D x H x W. As H and W are symmetrical, it is ok to just reverse.\n",
    "            new_spacing = tuple(map(operator.mul, spacing, change_factors))\n",
    "        else:\n",
    "            new_spacing = tuple([x/scale_factor for x in spacing])\n",
    "        t.set_spacing(new_spacing)\n",
    "\n",
    "    #if isinstance(t, TensorMask3D): mode = 'nearest'\n",
    "    if t.ndim == 3: t=t.unsqueeze(0)   # add fake chanel dim\n",
    "    if t.ndim == 4: t=t.unsqueeze(0)   # add fake batch dim\n",
    "\n",
    "    t = F.interpolate(t.float(), size=size,\n",
    "                         scale_factor=scale_factor,\n",
    "                         mode=mode,\n",
    "                         align_corners=align_corners,\n",
    "                         recompute_scale_factor=recompute_scale_factor).squeeze() #remove fake dims"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ef74ed",
   "metadata": {},
   "source": [
    "# Dataloaders\n",
    "\n",
    "TODO augmentations.\n",
    "\n",
    "- dset = tfms applied to items\n",
    "- splits into training/valid\n",
    "- bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54b7336f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs  = 2\n",
    "center_crop_sz = (50, 50, 50)\n",
    "#pad_sz = (150, 150, 150)\n",
    "# general_bbox = (40, 150, 100, 320, 0, 300)\n",
    "# general_bbox = (40, 150, 100, 320, 0, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f728dc35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 8, Valid: 2\n",
      "<class 'tuple'> 2 torch.Size([2, 1, 50, 50, 50]) torch.Size([2, 1, 50, 50, 50])\n",
      "4 1\n"
     ]
    }
   ],
   "source": [
    "# splits\n",
    "splits = RandomSplitter(seed=42)(items)\n",
    "print(f\"Training: {len(splits[0])}, Valid: {len(splits[1])}\")\n",
    "\n",
    "# tfms\n",
    "#tfms = [DoAll(), CropBBox(general_bbox)]\n",
    "tfms = [DoAll(), CenterCropTfm(center_crop_sz)]\n",
    "\n",
    "# tls\n",
    "tls = TfmdLists(items, tfms, splits=splits)\n",
    "\n",
    "# dls\n",
    "dls = tls.dataloaders(bs=bs, after_batch=AddChannel())\n",
    "\n",
    "# GPU\n",
    "dls = dls.cuda()\n",
    "\n",
    "# test get one batch\n",
    "b = dls.one_batch()\n",
    "print(type(b), len(b), b[0].shape, b[1].shape)\n",
    "print(len(dls.train), len(dls.valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c96e69",
   "metadata": {},
   "source": [
    "# Metric\n",
    "\n",
    "Linear combination of Dice and Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "entitled-supervision",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice(input, target):\n",
    "    iflat = input.contiguous().view(-1)\n",
    "    tflat = target.contiguous().view(-1)\n",
    "    intersection = (iflat * tflat).sum()\n",
    "    return ((2. * intersection) /\n",
    "           (iflat.sum() + tflat.sum()))\n",
    "\n",
    "def dice_score(input, target):\n",
    "    return dice(input.argmax(1), target)\n",
    "\n",
    "def dice_loss(input, target): \n",
    "    return 1 - dice(input.softmax(1)[:, 1], target)\n",
    "\n",
    "def loss(input, target):\n",
    "    return dice_loss(input, target) + nn.CrossEntropyLoss()(input, target[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9fcb4ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from mathiaspaul\n",
    "def dice_coeff(outputs, labels, max_label):\n",
    "    \"\"\"\n",
    "    Evaluation function for Dice score of segmentation overlap\n",
    "    \"\"\"\n",
    "    dice = torch.FloatTensor(max_label-1).fill_(0).to(outputs.device)\n",
    "    for label_num in range(1, max_label):\n",
    "        iflat = (outputs==label_num).view(-1).float()\n",
    "        tflat = (labels==label_num).view(-1).float()\n",
    "        intersection = (iflat * tflat).sum()\n",
    "        dice[label_num-1] = (2. * intersection) / (iflat.sum() + tflat.sum())\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db375a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipython nbconvert --to python  '6 - Dataloaders- NB - Simple-Copy1.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59acd0f9",
   "metadata": {},
   "source": [
    "# Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d86c1219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8717d790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backbone = efficientnet_b0 #r3d_18 (pretrained?)\n",
    "# learn    = unet_learner_3d(dls, backbone, n_out=2,  metrics = dice_score,\n",
    "#                           model_dir = \"./models\", cbs = [SaveModelCallback(monitor='dice_score')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "511d80b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backbone = efficientnet_b0 #r3d_18 (pretrained?)\n",
    "# learn    = unet_learner_3d(dls, backbone, n_out=2,  metrics = dice_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a16e375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unet_learner_3d??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4c918d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learner(\n",
    "#     dls,\n",
    "#     model,\n",
    "#     loss_func=None,\n",
    "#     opt_func=<function Adam at 0x7f0e60231dd0>,\n",
    "#     lr=0.001,\n",
    "#     splitter=<function trainable_params at 0x7f0e6b4618c0>,\n",
    "#     cbs=None,\n",
    "#     metrics=None,\n",
    "#     path=None,\n",
    "#     model_dir='models',\n",
    "#     wd=None,\n",
    "#     wd_bn_bias=False,\n",
    "#     train_bn=True,\n",
    "#     moms=(0.95, 0.85, 0.95),\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c74ffe07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 50, 50, 50])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b40167cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_val = b[0]\n",
    "img_val = F.avg_pool3d(img_val,5,stride=1,padding=2)\n",
    "img_val = F.avg_pool3d(img_val,5,stride=1,padding=2)\n",
    "img_val = F.avg_pool3d(img_val,3,stride=1,padding=1)\n",
    "_,_,D_in1,H_in1,W_in1 = img_val.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8ed1223a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50, 50)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D_in1,H_in1,W_in1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0af37e59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50, 50, 50])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_res = torch.tensor(center_crop_sz).long()\n",
    "full_res\n",
    "# full_res = torch.tensor(pad_sz).long()\n",
    "# full_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "26c53887",
   "metadata": {},
   "outputs": [],
   "source": [
    "#original OBELISK model as described in MIDL2018 paper\n",
    "#contains around 130k trainable parameters and 1024 binary offsets\n",
    "#most simple Obelisk-Net with one deformable convolution followed by 1x1 Dense-Net\n",
    "class obelisk_visceral_full(nn.Module):\n",
    "    def __init__(self,num_labels,full_res):\n",
    "        super(obelisk_visceral_full, self).__init__()\n",
    "        self.num_labels = num_labels\n",
    "        self.full_res = full_res\n",
    "        D_in1 = full_res[0]; H_in1 = full_res[1]; W_in1 = full_res[2];\n",
    "        D_in2 = (D_in1+1)//2; H_in2 = (H_in1+1)//2; W_in2 = (W_in1+1)//2; #half resolution\n",
    "        self.half_res = torch.Tensor([D_in2,H_in2,W_in2]).long(); half_res = self.half_res\n",
    "        D_in4 = (D_in2+1)//2; H_in4 = (H_in2+1)//2; W_in4 = (W_in2+1)//2; #quarter resolution\n",
    "        self.quarter_res = torch.Tensor([D_in4,H_in4,W_in4]).long(); quarter_res = self.quarter_res\n",
    "        \n",
    "        #Obelisk Layer\n",
    "        # sample_grid: 1 x    1     x #samples x 1 x 3\n",
    "        # offsets:     1 x #offsets x     1    x 1 x 3\n",
    "        \n",
    "        self.sample_grid1 = F.affine_grid(torch.eye(3,4).unsqueeze(0),torch.Size((1,1,quarter_res[0],quarter_res[1],quarter_res[2])))\n",
    "        self.sample_grid1.requires_grad = False\n",
    "        \n",
    "        #in this model (binary-variant) two spatial offsets are paired \n",
    "        self.offset1 = nn.Parameter(torch.randn(1,1024,1,2,3)*0.05)\n",
    "        \n",
    "        #Dense-Net with 1x1x1 kernels\n",
    "        self.LIN1 = nn.Conv3d(1024, 256, 1, bias=False, groups=4) #grouped convolutions\n",
    "        self.BN1 = nn.BatchNorm3d(256)\n",
    "        self.LIN2 = nn.Conv3d(256, 128, 1, bias=False)\n",
    "        self.BN2 = nn.BatchNorm3d(128)\n",
    "        \n",
    "        self.LIN3a = nn.Conv3d(128, 32, 1,bias=False)\n",
    "        self.BN3a = nn.BatchNorm3d(128+32)\n",
    "        self.LIN3b = nn.Conv3d(128+32, 32, 1,bias=False)\n",
    "        self.BN3b = nn.BatchNorm3d(128+64)\n",
    "        self.LIN3c = nn.Conv3d(128+64, 32, 1,bias=False)\n",
    "        self.BN3c = nn.BatchNorm3d(128+96)\n",
    "        self.LIN3d = nn.Conv3d(128+96, 32, 1,bias=False)\n",
    "        self.BN3d = nn.BatchNorm3d(256)\n",
    "        \n",
    "        self.LIN4 = nn.Conv3d(256, num_labels,1)\n",
    "\n",
    "        \n",
    "    def forward(self, inputImg, sample_grid=None):\n",
    "    \n",
    "        B,C,D,H,W = inputImg.size()\n",
    "        if(sample_grid is None):\n",
    "            sample_grid = self.sample_grid1\n",
    "        sample_grid = sample_grid.to(inputImg.device)    \n",
    "        #pre-smooth image (has to be done in advance for original models )\n",
    "        #x00 = F.avg_pool3d(inputImg,3,padding=1,stride=1)\n",
    "        \n",
    "        _,D_grid,H_grid,W_grid,_ = sample_grid.size()\n",
    "        input = F.grid_sample(inputImg, (sample_grid.view(1,1,-1,1,3).repeat(B,1,1,1,1) + self.offset1[:,:,:,0:1,:])).view(B,-1,D_grid,H_grid,W_grid)-\\\n",
    "        F.grid_sample(inputImg, (sample_grid.view(1,1,-1,1,3).repeat(B,1,1,1,1) + self.offset1[:,:,:,1:2,:])).view(B,-1,D_grid,H_grid,W_grid)\n",
    "        \n",
    "        x1 = F.relu(self.BN1(self.LIN1(input)))\n",
    "        x2 = self.BN2(self.LIN2(x1))\n",
    "        \n",
    "        x3a = torch.cat((x2,F.relu(self.LIN3a(x2))),dim=1)\n",
    "        x3b = torch.cat((x3a,F.relu(self.LIN3b(self.BN3a(x3a)))),dim=1)\n",
    "        x3c = torch.cat((x3b,F.relu(self.LIN3c(self.BN3b(x3b)))),dim=1)\n",
    "        x3d = torch.cat((x3c,F.relu(self.LIN3d(self.BN3c(x3c)))),dim=1)\n",
    "\n",
    "        x4 = self.LIN4(self.BN3d(x3d))\n",
    "        #return half-resolution segmentation/prediction \n",
    "        \n",
    "        #return x4\n",
    "        return F.interpolate(x4, size=[self.full_res[0], self.full_res[1], self.full_res[2]], mode='trilinear',align_corners=False)\n",
    "        #return F.interpolate(x4, size=[self.half_res[0],self.half_res[1],self.half_res[2]], mode='trilinear',align_corners=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e9193267",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import obeliskhybrid_visceral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a2c67140",
   "metadata": {},
   "outputs": [],
   "source": [
    "DiceLoss??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "913a35d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# D_in1 = imgs.size(2); H_in1 = imgs.size(3); W_in1 = imgs.size(4); #full resolution\n",
    "# full_res = torch.Tensor([D_in1,H_in1,W_in1]).long()\n",
    "\n",
    "# _,_,D_in1,H_in1,W_in1 = img_val.size()\n",
    "# full_res = torch.Tensor([D_in1,H_in1,W_in1]).long()\n",
    "\n",
    "learn = Learner(dls=dls, \\\n",
    "                model=obeliskhybrid_visceral(num_labels=2, full_res=full_res), \\\n",
    "                loss_func= DiceLoss(), #nn.CrossEntropyLoss(), \\\n",
    "                metrics = dice_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "89be5e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with learn.distrib_ctx():\n",
    "#     learn.fit_one_cycle(1, 3e-1, wd = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8de170fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU\n",
    "#learn.model = nn.DataParallel(learn.model)\n",
    "learn.model = learn.model.cuda()\n",
    "#learn = learn.to_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c345a3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed: 2.0201492309570312 s\n",
      "Batch: x,y\n",
      "<class 'torch.Tensor'> torch.Size([2, 1, 50, 50, 50]) torch.float32 \n",
      " <class 'torch.Tensor'> torch.Size([2, 1, 50, 50, 50]) torch.int64\n",
      "Pred shape\n",
      "<class 'torch.Tensor'> torch.Size([2, 2, 50, 50, 50]) torch.float32\n",
      "Loss\n",
      "tensor(0.4694, device='cuda:0', grad_fn=<MeanBackward0>)\n",
      "<faimed3d.models.losses.DiceLoss object at 0x7f6e095f57d0>\n"
     ]
    }
   ],
   "source": [
    "# test:\n",
    "\n",
    "#dls.device = \"cpu\"\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "x,y = dls.one_batch()\n",
    "#x,y = to_cpu(x), to_cpu(y)\n",
    "\n",
    "pred = learn.model(x)\n",
    "loss = learn.loss_func(pred, y)\n",
    "\n",
    "elapsed = time.time() - start\n",
    "\n",
    "print(f\"Elapsed: {elapsed} s\")\n",
    "print(\"Batch: x,y\")\n",
    "print(type(x), x.shape, x.dtype, \"\\n\", type(y), y.shape, y.dtype)\n",
    "\n",
    "print(\"Pred shape\")\n",
    "print(type(pred), pred.shape, pred.dtype)\n",
    "\n",
    "print(\"Loss\")\n",
    "print(loss)\n",
    "print(learn.loss_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "67e67dcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 1, 50, 50, 50]),\n",
       " torch.Size([2, 1, 50, 50, 50]),\n",
       " torch.Size([2, 2, 50, 50, 50]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape, pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c1c301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "52f35922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "# print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8962fb76",
   "metadata": {},
   "source": [
    "# LR Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c58b4d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4fefd197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE learn.fit one cycle\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>dice_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.447691</td>\n",
       "      <td>0.940130</td>\n",
       "      <td>0.114808</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"PRE learn.fit one cycle\")\n",
    "learn.fit_one_cycle(1, 3e-1, wd = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fd633caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unfreeze, learn 50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>dice_score</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.414686</td>\n",
       "      <td>0.535622</td>\n",
       "      <td>0.221014</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.388065</td>\n",
       "      <td>0.381264</td>\n",
       "      <td>0.349142</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.376620</td>\n",
       "      <td>0.350094</td>\n",
       "      <td>0.393100</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.364497</td>\n",
       "      <td>0.366397</td>\n",
       "      <td>0.389907</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.367923</td>\n",
       "      <td>0.377101</td>\n",
       "      <td>0.399783</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.357440</td>\n",
       "      <td>0.378759</td>\n",
       "      <td>0.412649</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.344720</td>\n",
       "      <td>0.370492</td>\n",
       "      <td>0.459223</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.332658</td>\n",
       "      <td>0.361295</td>\n",
       "      <td>0.511182</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.331257</td>\n",
       "      <td>0.357495</td>\n",
       "      <td>0.494907</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.317740</td>\n",
       "      <td>0.352300</td>\n",
       "      <td>0.488051</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.311530</td>\n",
       "      <td>0.364691</td>\n",
       "      <td>0.422893</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.297661</td>\n",
       "      <td>0.336416</td>\n",
       "      <td>0.543653</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.283907</td>\n",
       "      <td>0.326430</td>\n",
       "      <td>0.567546</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.275701</td>\n",
       "      <td>0.338488</td>\n",
       "      <td>0.528617</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.262246</td>\n",
       "      <td>0.333547</td>\n",
       "      <td>0.525227</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.253665</td>\n",
       "      <td>0.324192</td>\n",
       "      <td>0.518934</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.243160</td>\n",
       "      <td>0.296130</td>\n",
       "      <td>0.558563</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.231773</td>\n",
       "      <td>0.316965</td>\n",
       "      <td>0.504095</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.224475</td>\n",
       "      <td>0.354292</td>\n",
       "      <td>0.381580</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.216376</td>\n",
       "      <td>0.261958</td>\n",
       "      <td>0.582067</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.206861</td>\n",
       "      <td>0.255765</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.196324</td>\n",
       "      <td>0.194539</td>\n",
       "      <td>0.767722</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.188952</td>\n",
       "      <td>0.200936</td>\n",
       "      <td>0.753836</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.181817</td>\n",
       "      <td>0.229195</td>\n",
       "      <td>0.729876</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.173090</td>\n",
       "      <td>0.187899</td>\n",
       "      <td>0.801166</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.164066</td>\n",
       "      <td>0.177423</td>\n",
       "      <td>0.781455</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.156589</td>\n",
       "      <td>0.164661</td>\n",
       "      <td>0.785448</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.149533</td>\n",
       "      <td>0.154043</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.143358</td>\n",
       "      <td>0.153780</td>\n",
       "      <td>0.813607</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.136241</td>\n",
       "      <td>0.157700</td>\n",
       "      <td>0.813859</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.132506</td>\n",
       "      <td>0.166599</td>\n",
       "      <td>0.793584</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.126934</td>\n",
       "      <td>0.156578</td>\n",
       "      <td>0.815789</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.121248</td>\n",
       "      <td>0.166309</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.118604</td>\n",
       "      <td>0.171575</td>\n",
       "      <td>0.784452</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.113604</td>\n",
       "      <td>0.167044</td>\n",
       "      <td>0.795157</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.110860</td>\n",
       "      <td>0.168428</td>\n",
       "      <td>0.795455</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.106877</td>\n",
       "      <td>0.170071</td>\n",
       "      <td>0.797047</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.102705</td>\n",
       "      <td>0.164613</td>\n",
       "      <td>0.802930</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.100402</td>\n",
       "      <td>0.164648</td>\n",
       "      <td>0.797316</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.096329</td>\n",
       "      <td>0.164680</td>\n",
       "      <td>0.796776</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.094532</td>\n",
       "      <td>0.167020</td>\n",
       "      <td>0.796461</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.090980</td>\n",
       "      <td>0.166541</td>\n",
       "      <td>0.797159</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.089343</td>\n",
       "      <td>0.169197</td>\n",
       "      <td>0.793671</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.087496</td>\n",
       "      <td>0.169064</td>\n",
       "      <td>0.795887</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.084375</td>\n",
       "      <td>0.168468</td>\n",
       "      <td>0.793753</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.081451</td>\n",
       "      <td>0.167380</td>\n",
       "      <td>0.794164</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.080040</td>\n",
       "      <td>0.169030</td>\n",
       "      <td>0.791743</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.077483</td>\n",
       "      <td>0.168727</td>\n",
       "      <td>0.790476</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.075808</td>\n",
       "      <td>0.166283</td>\n",
       "      <td>0.791627</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.075558</td>\n",
       "      <td>0.167050</td>\n",
       "      <td>0.786979</td>\n",
       "      <td>00:07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"unfreeze, learn 50\")\n",
    "learn.unfreeze()\n",
    "learn.fit_one_cycle(50, 3e-3, wd = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fdc0b435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SuggestedLRs(lr_min=6.309573450380412e-08, lr_steep=6.309573450380412e-07)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7127658d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"unfreeze, learn 50\")\n",
    "# learn.unfreeze()\n",
    "# learn.fit_one_cycle(50, 1e-3, wd = 1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd40f6fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4191348",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5737f38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testmask = torch.tensor([[[False, False, False], [False, False, False], [True, True, True]],\n",
    "#                        [[False, False, False], [False, False, True], [True, True, True]],\n",
    "#                        [[False, False, False], [False, False, False], [False, False, False]]])\n",
    "# testmask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "a6a080bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testmaskN = np.array(testmask)\n",
    "# testmaskN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c1383dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maskT = testmask.type(torch.BoolTensor)\n",
    "\n",
    "# iT = torch.any(maskT, dim=(1,2))\n",
    "# jT = torch.any(maskT, dim=(0,2))\n",
    "# kT = torch.any(maskT, dim=(0,1))\n",
    "\n",
    "# iminT, imaxT = torch.where(iT)[0][[0, -1]]\n",
    "# jminT, jmaxT = torch.where(jT)[0][[0, -1]]\n",
    "# kminT, kmaxT = torch.where(kT)[0][[0, -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "64d3afac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maskN = np.array(testmask).astype(bool)\n",
    "    \n",
    "# iN = np.any(maskN, axis=(1, 2))\n",
    "# jN = np.any(maskN, axis=(0, 2))\n",
    "# kN = np.any(maskN, axis=(0, 1))\n",
    "\n",
    "# iminN, imaxN = np.where(iN)[0][[0, -1]]\n",
    "# jminN, jmaxN = np.where(jN)[0][[0, -1]]\n",
    "# kminN, kmaxN = np.where(kN)[0][[0, -1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "1201e773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# maskT.shape, maskN.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "8611c9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(iT)\n",
    "# print(jT)\n",
    "# print(kT)\n",
    "# print([x for x in (iminT, imaxT, jminT, jmaxT, kminT, kmaxT)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "7d428ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(iN)\n",
    "# print(jN)\n",
    "# print(kN)\n",
    "# print([int(x) for x in (iminN, imaxN, jminN, jmaxN, kminN, kmaxN)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93e81e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#     def torch_mask2bbox(mask):\n",
    "#         mask = mask.type(torch.BoolTensor)\n",
    "\n",
    "#         i = torch.any(mask, dim=0)\n",
    "#         j = torch.any(mask, dim=1)\n",
    "#         k = torch.any(mask, dim=2)\n",
    "\n",
    "#         imin, imax = torch.where(i)[0][[0, -1]]\n",
    "#         jmin, jmax = torch.where(j)[0][[0, -1]]\n",
    "#         kmin, kmax = torch.where(k)[0][[0, -1]]\n",
    "\n",
    "#         # inclusive idxs\n",
    "#         return imin, imax+1, jmin, jmax+1, kmin, kmax+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
